
# ğŸ§  LLM Model Selector (Web UI + Docker)

A simple, intuitive web-based tool to help you choose the best Large Language Model (LLM) variant for **local inference** based on:
- Use case (chat, code, rag, thinking)
- Hardware specs (GPU VRAM, context window)
- Performance needs (tokens per minute)
- Optimization features (Flash Attention, KV Cache)

Built with [Flask](https://flask.palletsprojects.com/) and runs easily via Docker.

## ğŸ“¦ Features

- âœ… Web UI (access from any browser)
- âœ… Suggests top 10 model variants
- âœ… Optional toggles for Flash Attention & KV Cache
- âœ… Dockerized for easy deployment
- âœ… Extendable for Hugging Face integration or Ollama setup

## ğŸ—‚ï¸ Folder Structure

```
llm_model_selector_web/
â”œâ”€â”€ app.py               # Main Flask web app
â”œâ”€â”€ Dockerfile           # Docker setup for running the web app
â”œâ”€â”€ requirements.txt     # Python package dependencies
â””â”€â”€ README.md            # You're reading it!
```

## ğŸš€ Quick Start (Docker)

```bash
git clone https://github.com/yourusername/llm_model_selector_web.git
cd llm_model_selector_web

docker build -t model-selector .
docker run -d -p 8500:8500 --name model-selector model-selector
```

Then open your browser at: [http://localhost:8500](http://localhost:8500)

## ğŸ“¥ Requirements (if running locally)

```bash
pip install flask
python app.py
```

## ğŸ”® Planned Features

- Hugging Face model search and links
- Ollama-compatible GGUF recommendations
- LM Studio integration
- Local model loader helper

## ğŸ¤ Contributing

Contributions welcome! Fork the repo, make changes, and submit a pull request.

## ğŸ“„ License

MIT License. Use freely with attribution.
